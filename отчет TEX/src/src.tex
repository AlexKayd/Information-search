\section{Сбор корпуса документов}

В качестве источника информации были выбраны три русскоязычных сайта, посвящённых вопросам материнства и воспитания детей: \texttt{7ya.ru}, \texttt{mama.ru}, \texttt{letidor.ru}. Эти ресурсы содержат большое количество структурированных текстов, что делает их пригодными для автоматического анализа.

Чтобы удостовериться, что выбранные источники подходят для лабораторных, в поисковой строке Google я ввела запрос: \texttt{«первый прикорм схема site:letidor.ru OR site:7ya.ru OR site:mama.ru»}. Google выдал страницы, соответствующие запросу. Значит, мой корпус документов уже проиндексирован и доступен для поиска через мощную существующую систему.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/google_search.png}
\caption{Поиск в Google}
\end{figure}

Недостатки существующих поисковиков:
\begin{itemize}
    \item Поисковые системы сложны в использовании для тех, кто не владеет навыками составления запросов;
    \item Чаще показывается то, что популярно или хорошо раскручено, а не то, что лучше всего отвечает на ваш конкретный вопрос.
\end{itemize}

Для сбора данных был разработан поисковой робот, выполняющий следующие функции:

\begin{enumerate}
    \item \textbf{Обход целевых доменов}. Обход осуществляется двумя способами:
    \begin{itemize}
        \item Рекурсивный обход внутренних ссылок с начальных URL-адресов;
        \item Парсинг \texttt{sitemap.xml}, с автоматическим извлечением всех вложенных карт сайта.
    \end{itemize}
    
    \item \textbf{Фильтрация и валидация URL}. Ссылка считается допустимой только при соответствии шаблонам. Для каждого сайта свой шаблон, полученный путем анализа. Все остальные ссылки игнорируются. Также проверяется соответствие \texttt{robots.txt} с кэшированием на 24 часа.
    
    \item \textbf{Извлечение структурированных данных}. Для каждой статьи извлекаются:
    \begin{itemize}
        \item заголовок, автор, дата публикации;
        \item основной текст;
        \item исходный HTML.
    \end{itemize}
    
    \item \textbf{Очистка и фильтрация контента}. Перед сохранением:
    \begin{itemize}
        \item Удаляются нерелевантные элементы;
        \item Проверяется наличие ошибок (404, страница не найдена) по заголовку, h1, содержимому;
        \item Пропускаются документы с менее чем 10 словами или пустым основным текстом.
    \end{itemize}
    
    \item \textbf{Структурированное хранение}. Все данные сохраняются в базу данных PostgreSQL в таблицу \texttt{documents} с полями:
    \begin{itemize}
        \item \texttt{url}, \texttt{normalized\_url} — исходный и нормализованный URL;
        \item \texttt{title}, \texttt{author}, \texttt{publish\_date}, \texttt{clean\_text}, \texttt{html};
        \item \texttt{source} — имя домена;
        \item \texttt{fetch\_timestamp}, \texttt{last\_fetch\_attempt} — метки времени загрузки и последней попытки обновления;
        \item \texttt{last\_modified\_header}, \texttt{etag\_header} — HTTP-заголовки для эффективного обновления.
    \end{itemize}
    
    \item \textbf{Обновление ранее собранных документов}. После завершения начального сбора запускается фаза обновления:
    \begin{itemize}
        \item Выбираются документы, у которых \texttt{last\_fetch\_attempt} старше 7 дней;
        \item Для каждого выполняется условный HTTP-запрос;
        \item При ответе 304 Not Modified обновляется только дата попытки;
        \item При изменении контента — документ перезаписывается с новыми данными;
        \item При редиректе — проверяется, не существует ли уже новая версия; при конфликте — обновление отменяется.
    \end{itemize}
    
    \item \textbf{Устойчивость и прерывание}.
    \begin{itemize}
        \item Состояние обхода документа сохраняется в файл \texttt{crawler\_state.pkl} после каждого шага;
        \item При получении сигнала Ctrl+C программа завершается корректно, сохраняя текущее состояние.
    \end{itemize}
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.87\textwidth]{images/crawler_example.png}
\caption{Пример работы поискового робота}
\end{figure}

В результате работы краулера был сформирован корпус, содержащий \textbf{54 543} документа.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/site_distribution.png}
\caption{Распределение количества документов по сайтам}
\end{figure}

\begin{table}[H]
\centering
\caption{Информация по данным в БД (значения получены путем запросов к 1000 рандомным документам из БД).}
\begin{tabular}{|l|l|}
\hline
\textbf{Количество документов} & 54 543 \\
\hline
\textbf{Источники} & 7ya.ru, mama.ru, letidor.ru \\
\hline
\textbf{Средний размер сырого HTML} & 304.625 КБ \\
\hline
\textbf{Средний размер извлечённого текста} & 3 192 символов, 5.66 КБ \\
\hline
\textbf{Общий объём БД} & 8 ГБ \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/db_schema.png}
\caption{Поля в таблице documents}
\end{figure}

\section{Предварительная обработка текста}

\subsection{Экспорт текстов из базы данных}

Перед токенизацией тексты извлекаются из таблицы \texttt{documents} с помощью вспомогательного скрипта \texttt{export\_clean\_text.py}. Каждый документ сохраняется в отдельный файл \texttt{docs/id.txt}, где \texttt{id} — уникальный идентификатор документа в PostgreSQL.

\subsection{Токенизация}

Токенизация реализована в программе \texttt{tokenizer.exe}, написанной на C++17 с использованием UTF-8 обработки кириллицы. Программа читает все файлы из \texttt{docs/}, применяет правила фильтрации и сохраняет результат в \texttt{tokens/id.tokens}.

Основные правила токенизации:
\begin{itemize}
    \item Разрешены только русскоязычные слова и чистые числа до 4 цифр. Английские слова, смеси символов, спецсимволы и HTML-сущности отбрасываются.
    \item Слова ограничены по длине, максимум 20 UTF-8-символов.
    \item Дефис разрешён только один, и только если по обе стороны от него корректные русские части, каждая не менее 2 кириллических символов, например: \texttt{физико-математический}.
    \item Повторы символов автоматически отфильтровываются.
    \item Регистр приводится к нижнему.
    \item Известные аббревиатуры сохраняются в исходном виде из файла \texttt{known\_abbrevs.txt}.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/tokenizer_output.png}
\caption{Результаты запуска токенизации}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/tokens_example.png}
\caption{Пример токенов}
\end{figure}

\subsection{Стемминг}

После токенизации запускается программа \texttt{stemmer.exe}, которая читает файлы из \texttt{tokens/} и применяет правила нормализации к каждому токену, сохраняя результат в \texttt{stems/id.stems}.

Особенности стеммера:
\begin{itemize}
    \item Удаление стоп-слов перед стеммингом, список загружается из \texttt{stop\_words.txt}. Это позволяет сократить размер индекса и улучшить релевантность.
    \item Обработка возвратных глаголов: суффиксы \texttt{-ся}/\texttt{-сь} корректно удаляются.
    \item Специальные правила для русского языка.
    \item Исключения: слова-глаголы высокой частотности (\texttt{быть}, \texttt{мочь}) остаются без изменений.
    \item Минимальная длина стемы: стемы короче 2 UTF-8 символов отбрасываются.
    \item Все суффиксы отсортированы по убыванию длины, чтобы сначала применять наиболее специфичные правила.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/stemmer_output.png}
\caption{Результаты запуска стеммера}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{images/stems_example.png}
\caption{Пример стем}
\end{figure}

\section{Закон Ципфа}

Закон Ципфа — наблюдение, согласно которому частота встречаемости слова в тексте обратно пропорциональна его рангу в убывающем списке частот.

Для проверки закона был составлен список всех токенов корпуса с их частотами. Токены отсортированы по убыванию частоты. Построен график зависимости частоты токена от его ранга в логарифмических координатах.

\begin{figure}[H]
\centering
\includegraphics[width=0.99\textwidth]{images/zipf_law_graph.png}
\caption{Закон Ципфа для собранного корпуса}
\end{figure}

График демонстрирует чёткую линейную зависимость в логарифмическом масштабе, это является подтверждением закона Ципфа. Это означает, что:
\begin{itemize}
    \item Небольшое количество слов встречаются очень часто (верхняя часть графика);
    \item Огромное число слов встречается крайне редко (нижняя часть графика).
\end{itemize}

Наличие небольших отклонений в области высоких рангов (правая часть графика) — нормальное явление, обусловленное шумом и конечным объёмом корпуса.

\section{Булев индекс}

Булев индекс реализован как единая обратная индексная структура, оптимизированная для поддержки булевых запросов. Индекс строится на основе уже нормализованных и отфильтрованных стем из папки \texttt{stems/}. Для каждого уникального термина формируется posting-лист — отсортированный список идентификаторов документов, в которых встречается данный термин.

Особенности реализации:
\begin{itemize}
    \item Удаление дубликатов на уровне документа: если термин встречается в тексте несколько раз, в posting-лист он добавляется только один раз.
    \item Лексикографическая сортировка терминов: все термины в индексе упорядочены по алфавиту, что упрощает последующий поиск и валидацию.
    \item Сортировка posting-листов по возрастанию ID: каждый список документов отсортирован с помощью алгоритма сортировки вставками.
    \item Удаление дубликатов в posting-листах: после сортировки из списков удаляются повторяющиеся идентификаторы.
    \item Сохранение в текстовом формате: индекс записывается в файл \texttt{boolean\_index.txt} в виде \texttt{термин:doc1,doc2,doc3,...}.
\end{itemize}

Индекс строится однократно и сохраняется на диск (\texttt{boolean\_index.txt}). При последующих запусках поисковой системы он загружается напрямую.

\begin{figure}[H]
\centering
\includegraphics[width=1.04\textwidth]{images/boolean_index_output.png}
\caption{Результаты запуска булевого индекса}
\end{figure}

\section{Булев поиск}

Реализованная поисковая система поддерживает булевы запросы с использованием следующих операторов:
\begin{itemize}
    \item \texttt{and} — логическое И (документ должен содержать все указанные термины);
    \item \texttt{or} — логическое ИЛИ (документ должен содержать хотя бы один термин);
    \item \texttt{and not} — исключение (документ содержит первый термин, но не содержит второй).
\end{itemize}

Запросы вводятся через консоль, например: \texttt{мама and ребёнок}.

Этапы алгоритма поиска:
\begin{enumerate}
    \item Преобразование запроса: все термины приводятся к нижнему регистру для соответствия формату индекса.
    \item Поиск терминов: для каждого термина выполняется бинарный поиск в списке \texttt{boolean\_index.txt}.
    \item Извлечение posting-листов: если термин найден, из индекса извлекается отсортированный список идентификаторов документов.
    \item Применение логических операций:
    \begin{itemize}
        \item Для оператора \texttt{and} выполняется пересечение двух списков (алгоритм двух указателей, линейная сложность);
        \item Для \texttt{or} — объединение с удалением дубликатов;
        \item Для \texttt{and not} — разность множеств (документы из первого списка, отсутствующие во втором).
    \end{itemize}
    \item Формирование результата:
    \begin{itemize}
        \item В режиме \texttt{--ids-only} выводятся только идентификаторы документов;
        \item В обычном режиме система подключается к базе данных PostgreSQL и извлекает заголовок и нормализованный URL каждого найденного документа.
    \end{itemize}
\end{enumerate}

Все списки документов в индексе отсортированы по возрастанию, что позволяет выполнять операции пересечения и объединения за линейное время без дополнительной сортировки.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{images/query_normal_1.png}
\caption{Пример запросов в обычном режиме}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.1\textwidth]{images/query_normal_2.png}
\caption{Пример запросов в обычном режиме}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.1\textwidth]{images/query_normal_3.png}
\caption{Пример запросов в обычном режиме}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.35\textwidth]{images/query_ids_only.png}
\caption{Примеры запросов в --ids-only режиме}
\end{figure}